<!DOCTYPE html><html><head><meta content="IE=edge" http-equiv=X-UA-Compatible><meta charset=utf-8><meta name=description content="北市真の日記"><meta content="width=device-width,initial-scale=1.0" name=viewport><title>アペフチ</title><script src="../../bower_components/webcomponentsjs/webcomponents-lite.js"></script><link href="../../components/elements.vulcanized.html" rel=import /><link rel=alternate type="application/atom+xml" title="アペフチ" href="../../recent-days.atom"/><link href="../../bower_components/sanitize-css/sanitize.css" rel=stylesheet /><link href="../../stylesheets/highlight-81ca03d6.css" rel=stylesheet /><script src="../../javascripts/all-d7c52391.js"></script><link href="../../favicon.ico" rel="shortcut icon" type="image/x-icon"/><link href="../../images/favicon-8d71db77.png" rel=icon type="image/png" size=60x60 /><link href="/apehuci/manifest.json" rel=manifest><meta name=theme-color content="#b71c1c"><meta name=og:description content="北市真の日記"><meta name=og:title content="アペフチ"><meta name=twitter:card content=summary><meta name=twitter:site content=@KitaitiMakoto><meta name=twitter:image content="https://kitaitimakoto.github.io/apehuci/images/icon-bbe8ac52.png"><meta name=robots content="noindex,follow"></head><body class="page page_9 page_9_index"><blog-router base-regex="^/apehuci/"></blog-router><main><app-header-layout><app-header fixed condenses effects=waterfall slot=header><app-toolbar></app-toolbar><app-toolbar sticky><h1 class=site-title><a href="../../">アペフチ</a></h1></app-toolbar></app-header><article><paper-card elevation=2><a href="../../2016/02/27.html"><h1>日記の検索部分をHTTP/2対応した</h1><time pubdate>2016-02-27</time></a><ul class=tags><li><a href="../../tags/http-2.html" rel=tag>http/2</a></li><li><a href="../../tags/groonga.html" rel=tag>Groonga</a></li></ul><p>この日記の検索機能（フッターの検索フォームからできる）では<a href="http://groonga.org/ja/docs/reference/executables/groonga-httpd.html">groonga-httpd</a>を使っている。日記本文は<a href="https://pages.github.com/">GitHub Pages</a>にホストしてもらっている。GitHub PagesはHTTPでもHTTPSでもどちらでもアクセスできて、ツイッターなどでURIを貼る時には僕はHTTPSの方を使っている。だがgroonga-httpdはこれまでHTTPSに対応していなかったので、<a href="https://developer.mozilla.org/ja/docs/Web/API/XMLHttpRequest">XHR</a>で接続することができなかった。仕方なく前段に<a href="http://nginx.org/">Nginx</a>を立てて、そこでTLSの終端をしていた。</p><p>日課の<code class=highlighter-rouge>apt-get update &amp;&amp; apt-get upgrade</code>をしていたらGroongaの各種パッケージが降って来たので更新内容を確認しに行った（<a href="http://groonga.org/ja/docs/news.html#release-6-0-0">6.0.0リリース - 2016-02-29</a>）。そこにTLSサポートのことが書かれていたので、早速この日記の検索サーバーでもアップデートして、TLSを有効にした（と書くと白々しいか、TLSサポートは僕が要望した物だった：<a href="https://osdn.jp/projects/groonga/lists/archive/dev/2016-February/003951.html">https://osdn.jp/projects/groonga/lists/archive/dev/2016-February/003951.html</a>）。</p><p>また、groonga-httpdは前々からHTTP/2が使えるようになっていたので、ついでにそちらも有効にした。</p><p>検索は単純にキーアップイベントを拾い、入力の一文字目からその都度groonga-httpdに検索リクエストを投げるようにしているので、HTTP/2でコネクションを張りっぱなしにして検索できるのは効果が大きいのではないかと思う（測ってない）。</p><p>ただ、検索用に日記のデータを入力するのは手元のスクリプトで<a href="https://github.com/ranguba/groonga-client">groonga-client</a> gemを使って実行していたのだが、このgemがHTTPS対応していないのでそこを対応しないといけない。それまでは、今日のこの日記以降の記事は検索対象にならない。HTTP接続用の別のポートを開けてもいいが、まあいいだろう。</p></paper-card></article><article><paper-card elevation=2><a href="../../2016/02/26.html"><h1>Groongaでできる検索方法あれこれ</h1><time pubdate>2016-02-26</time></a><ul class=tags><li><a href="../../tags/groonga.html" rel=tag>Groonga</a></li><li><a href="../../tags/.html" rel=tag>全文検索</a></li></ul><p><a href="https://groonga.doorkeeper.jp/events/39274">Groongaで学ぶ全文検索 2016-02-26</a>に行って来た。</p><p>今日のお題は「select」。<a href="http://groonga.org/ja/docs/reference/commands/select.html">select</a>というのは、Groongaの、全文検索エンジンとして最も重要な機能である検索を行うためのコマンド。</p><p>前回、実際のテーブルやデータを見ながら話せたのがよかったという声が多かったので、今回も具体的なGroongaの使い方を、というところからの選択。</p><p>まず僕が使っている以下の機能を簡単に紹介したあとで、それ以外のを<a href="https://twitter.com/ktou">@ktou</a>さんが紹介するという流れだった。</p><p>僕が使っている機能：</p><ul><li>全文検索</li><li>完全一致検索 … <a href="https://github.com/ranguba/epub-searcher">EPUB Searcher</a>の削除機能でレコードを特定する時に使っている</li><li>snippet_html出力 … この日記で、検索結果中の検索語をハイライトしている</li><li>グルーピング（ドリルダウン） … <a href="https://github.com/ranguba/epub-searcher">EPUB Searcher</a>で著者一覧ページで、著者の下に本一覧を表示するのに使っている</li><li>カラムの重み付け … この日記の検索で、本文よりタイトルとタグでのヒットを重視している。</li></ul><p>以下、@ktouさんが紹介してくれた機能：</p><h2 id=section>クエリー言語として解析</h2><p>例えばGoogleで「A B C」で検索すると「Aが含まれるかつBが含まれるかつCが含まれるページを探す」という意味になる。「A or B or C」と検索すると「Aが含まれるまたはBが含まれるまたはCが含まれるページを探す」という意味になる。ツイッターで「groonga -lang:ja」みたいな検索を僕はよくしていて、ここでの「lang:ja」（日本語のツイート）「-」（を除外する）というのもある<ins>（Groongaでもこの記法そのまま使えるとのこと。<code class=highlighter-rouge>lang:ja</code>が<code class=highlighter-rouge>lang</code>テーブルに対する<code class=highlighter-rouge>ja</code>での検索になり、「<code class=highlighter-rouge>-</code>」を付けると除外になる）</ins>。こうした、クエリーの中に検索語以外の命令を含められる時、この命令を表現する物がクエリー言語と呼ばれる。Groongaにもこのクエリー言語を理解する能力がある。</p><h2 id=section-1>集計機能</h2><p>グルーピングした際、グループそれぞれについて、何件のレコードがヒットしたかを集計することもできる。件数を数えるほか、あるカラムの値の合計、平均、最大値、最小値を求めることもできる。</p><p>忘れていたけど、この、件数を取得して表示するのはEPUB Searcherで使っていた。</p><h2 id=section-2>クエリー展開</h2><p>「焼き肉」で検索した時、勝手に「焼き肉 OR 焼肉 OR やきにく」というクエリーに変更して検索してくれる機能。「焼き肉と焼肉とやきにくを同一視する」ということ自体はユーザー（アプリケーション開発者）が予め登録しておく必要がある（どうやって？）。何を同一視したいかというのは、アプリケーションによって変わるからだ。</p><h2 id=section-3>重みの底上げ</h2><p>いい名前が付いていないと言っていた。</p><p>検索時にカラムごとに重みを設定して（タイトルは重くする、本文はそれほどでもない、など）最終的にレコードごとに総合スコアを出す。これが通常の重み付け。</p><p>その総合スコアの計算の<em>後に</em>、更に重みの数値を足すことができる。例えば、キャンペーンフラグカラムがONだと重み+10する、など。</p><p>通常の重み付けはカラムに対しての指定だが、この底上げはレコード一つ一つに対しての指定となる点が特徴となる。</p><p>使い方例も紹介されたが公開していいかどうか分からないとのことだったのでここでは他の例で説明する。Google検索では、過去に検索して閲覧したことがあるページが、次からの検索で上位になりやすいようになっている。これは、「閲覧した回数」といったカラムを持っておいて、総合スコアを計算し終わった後にそのカラムの値を追加して検索上位に持ってくる、というふうに、Groongaを使う場合は実装することができる。</p><h2 id=section-4>ソート</h2><p>ソートできる。</p><h2 id=section-5>途中から結果を出せる</h2><p>オフセットとかページネーションとかいうやつ。</p><h2 id=section-6>参照先でも検索できる</h2><p>SQLだとJOINするようなのがGroongaはJOINせずに検索できる。</p><p>記事テーブルとコメントテーブルがあって、記事に対してコメントが結び付いているとする。この時、SQLだと記事の主キーとコメントの主キーを<code class=highlighter-rouge>JOIN</code>で結び付ける。</p><ol><li>記事とコメントがくっついたテーブルを作る（と見做すことができる）</li><li>コメントテーブルの該当カラムに対して検索を実施し、</li><li>「くっついたテーブル」の記事テーブルの使いたいカラムを取得する</li></ol><p>となる。</p><p>Groongaは<del>コメントテーブルに記事への参照を</del><ins>記事テーブルにコメントへの参照を（発表していて逆だねって指摘してもらった）</ins>直接保持させることができる。だから、<code class=highlighter-rouge>JOIN</code>のような追加の命令無く</p><ol><li>コメントに対して検索し、</li><li>そのコメントが付いている記事を取得する</li></ol><p>ということができる。</p><p>記事に対して、後から関連商品みたいなテーブルを加えたくなることがあるかも知れない。この時、RDBMSであれば（正規化されていれば）単にそのためのテーブルを作成し、検索時に<code class=highlighter-rouge>JOIN</code>で結び付けるようにすることができる。記事テーブルには変更が必要ない。</p><p>Groongaの場合は記事テーブルに関連商品テーブルへの参照を持たせることになる（か逆方向の参照を持たせる。方向はアプリケーションによる）。既存の記事テーブルに対する変更が発生する。しかし、Groongaはカラム指向データベースなので、こうしたカラムの追加操作は低コストでできるので問題にならない（RDBMSで<code class=highlighter-rouge>ALTER TABLE</code>となると、件数によってはおおごとだ）。</p><p>「Solrはこうしたテーブル跨ぎの検索はうまくできないんでは？」と@ktouさんが言っていたので識者の指摘を待ちたい。</p></paper-card></article><article><paper-card elevation=2><a href="../../2016/02/21.html"><h1>A Tour of Goのエクササイズをやってみた</h1><time pubdate>2016-02-21</time></a><ul class=tags><li><a href="../../tags/go.html" rel=tag>Go</a></li></ul><p><a href="https://tour.golang.org/">A Tour of Go</a>のエクササイズをやって、GitHubに上げてみた：<a href="https://github.com/KitaitiMakoto/a-tour-of-go-exercises">github.com/KitaitiMakoto/a-tour-of-go-exercises</a>。</p><p>ウェブクローラーの課題（<a href="https://github.com/KitaitiMakoto/a-tour-of-go-exercises/blob/master/exercise-web-crawler.go">exercise-web-crawler.go</a>）が難しくて、まずgoroutineを使って非同期にクロールさせるのに手こずった。一つのHTTPリクエストに一つのgoroutineを割り当てた時、終了の待ち合わせはどうするのがいいんだろう。僕は、一々チャンネルを閉じるようにした。閉じるのの待ち合わせは</p><div class="language-go highlighter-rouge"><pre class=highlight><code><span class="k">for</span><span class="x"> </span><span class="k">range</span><span class="x"> </span><span class="n">ch</span><span class="x"> </span><span class="p">{}</span><span class="x">
</span></code></pre></div><p>とやった。この他に、<code class=highlighter-rouge>go</code>呼び出しの辺りにラベルを付けて、groutineから戻ったところで<code class=highlighter-rouge>break</code>するというやり方もあるようだ。エクササイズは本当は教師がいて答え合わせしてもらえるととてもいいのだけど、残念ながらいないので、誰か、「こうしたらもっといいよ」というの教えてください。まあ、色んなソースを読むというのが、よいのだろうとは思うが。</p><p>次に、「一度フェッチしたURIは二度フェッチしないようにする」というのも課題の一部で、ヒントに「その管理にマップを使うのはいいけど、マップは単独では並行処理に関して安全ではない」とあって、この排他制御にもちょっと困った。大きなロックを獲得して、その中でフェッチすると、並行処理させている意味が無くなっちゃう。でも、どのタイミングでロックを取ればいいのか、何のロックを取ればいいのか、というのに迷った。「一つのURIにつき一つのミューテックスを作る」ということを最初考えたのだけど、そもそもあるURIに対応するミューテックスが存在するかの確認処理と、その後ミューテックスを作るまでの間に他のgoroutineが同じ物を触る可能性があるわけで、うまくいかない。結局、単純にマップその物をロックするようにした。そうすると、<code class=highlighter-rouge>defer</code>を使わない実装になってしまったのだけど、もっといいやり方がないものだろうか。</p></paper-card></article><footer><ul><li class=next><a href="../8/" rel=next><paper-button raised>前3日分</paper-button></a></li><li class=prev><a href="../10/" rel=prev><paper-button raised>次3日分</paper-button></a></li></ul><paper-card><blog-search url="https://search.apehuci.kitaitimakoto.net/d" base=/apehuci/ table=Apehuci debounce-duration=300></blog-search></paper-card><paper-card> &copy; <a class=copy href="https://twitter.com/KitaitiMakoto">北市真</a></paper-card></footer></app-header-layout></main><script async="" src="//platform.twitter.com/widgets.js" charset=utf-8></script></body></html>